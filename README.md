# CIFAR-10 Machine Learning Final Project

This project is the final deliverable for a university Machine Learning course.  
It focuses on applying **classical machine learning techniques** to the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, along with a **CNN baseline** for performance comparison.

>  This project avoids relying on deep learning as a primary method and instead emphasizes classical ML pipelines, feature extraction, dimensionality reduction, and evaluation.

---

##  Project Goals

- Classify images in the CIFAR-10 dataset using machine learning.
- Apply preprocessing steps: **HOG** (Histogram of Oriented Gradients) and **PCA** (Principal Component Analysis).
- Train and evaluate multiple ML models.
- Tune hyperparameters of each model.
- Compare results to a basic CNN baseline.

---

## ğŸ“ Project Structure

```
Cifar10-ML-Final-Project/
â”‚
â”œâ”€â”€ data/                          # (Optional) Place to store raw or preprocessed data manually
â”‚
â”œâ”€â”€ models/                        # Machine learning and CNN model definitions
â”‚   â”œâ”€â”€ knn.py
â”‚   â”œâ”€â”€ svm.py
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â””â”€â”€ cnn_baseline.py
â”‚
â”œâ”€â”€ preprocessing/                 # Feature extraction and preprocessing utilities
â”‚   â”œâ”€â”€ hog.py
â”‚   â”œâ”€â”€ pca.py
â”‚   â””â”€â”€ combine_features.py
â”‚
â”œâ”€â”€ evaluation/                    # Metrics, confusion matrices, summary evaluators
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ confusion_analysis.py
â”‚   â””â”€â”€ model_summary.py
â”‚
â”œâ”€â”€ tuning/                        # Hyperparameter tuning scripts and plots
â”‚   â”œâ”€â”€ tune_knn.py
â”‚   â”œâ”€â”€ tune_svm.py
â”‚   â”œâ”€â”€ tune_rf.py
â”‚   â””â”€â”€ tune_logreg.py
â”‚
|
â”‚ 
â”‚
â”œâ”€â”€ report/                        # Final LaTeX report and compiled PDF
â”‚   â”œâ”€â”€ main.tex
â”‚   â”œâ”€â”€ raw_results_table.tex
â”‚   â”œâ”€â”€ pca_results_table.tex
â”‚   â”œâ”€â”€ hog_pca_results_table.tex
â”‚   â”œâ”€â”€ cnn_results_table.tex
â”‚   â”œâ”€â”€ model_summary_table.tex
â”‚   â”œâ”€â”€ tuning_knn_plot.png
â”‚   â”œâ”€â”€ tuning_svm_plot.png
â”‚   â”œâ”€â”€ tuning_rf_plot.png
â”‚   â”œâ”€â”€ tuning_logreg_plot.png
â”‚   â””â”€â”€ ML_Project_Report.pdf
â”‚
â”œâ”€â”€ main.py                        # Main script to run the full pipeline
â”œâ”€â”€ requirements.txt               # All required dependencies
â””â”€â”€ README.md                      # Project overview and instructions

```

---

##  Requirements

You can install all dependencies via:

```bash
pip install -r requirements.txt
```

### Main libraries used:
- `numpy`, `scikit-learn`, `matplotlib`, `pandas`
- `opencv-python` for HOG
- `torch`, `torchvision` for the CNN

---

##  How to Run the Project

After installing requirements:

```bash
python main.py
```

This will:
1. Run **HOG + PCA** on CIFAR-10 images
2. Train and evaluate:
   - Logistic Regression
   - K-Nearest Neighbors
   - Random Forest
   - Support Vector Machine
3. Train a **simple CNN baseline**
4. Output performance results and summary metrics

All results will be logged in the `report/` directory.

---

##  Dimensionality Reduction

- **PCA** is used after HOG to reduce dimensionality.
- `n_components=100` was chosen (explained in the report).
- This helps models train faster and generalize better.

---

##  Models & Evaluation

Each classical model was evaluated using:
- **Accuracy**, **Precision**, **Recall**, **F1 Score**
- **Confusion Matrix** per class

Additionally:
- Each model was **fine-tuned** on its 2 most important hyperparameters.
- CNN was used **only as a baseline**, not as the projectâ€™s focus.

---

##  Project Outputs

- Evaluation tables for all models (before/after PCA-HOG)
- Comparison to CNN
- Hyperparameter tuning logs
- Simulated full-dataset results (used due to resource limits)

---

## ğŸ’¡ Notes

- This project simulates full CIFAR-10 results due to memory constraints.
- All optimizations (PCA, HOG) are applied together.
- CNN is not optimized â€” it serves as a lower-bound reference.

---

## âœï¸ Report

The final report will include:
- Explanation of decisions, challenges, and optimizations
- Visualization of hyperparameter tuning
- Model performance summary table
- Discussion of results and insights per class

---

##  Credits

- CIFAR-10 dataset: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
- Instructor: *Professor Lee-Ad Gottlieb / School of Computer Science*
- Student: **Nevo Biton**

---

##  Questions?

Feel free to open an issue or contact [NevoBiton20](https://github.com/NevoBiton20).
