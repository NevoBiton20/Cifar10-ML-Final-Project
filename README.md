# CIFAR-10 Machine Learning Final Project

This project is the final deliverable for a university Machine Learning course.  
It focuses on applying **classical machine learning techniques** to the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, along with a **CNN baseline** for performance comparison.

> ğŸ“Œ This project avoids relying on deep learning as a primary method and instead emphasizes classical ML pipelines, feature extraction, dimensionality reduction, and evaluation.

---

## ğŸ” Project Goals

- Classify images in the CIFAR-10 dataset using machine learning.
- Apply preprocessing steps: **HOG** (Histogram of Oriented Gradients) and **PCA** (Principal Component Analysis).
- Train and evaluate multiple ML models.
- Tune hyperparameters of each model.
- Compare results to a basic CNN baseline.

---

## ğŸ“ Project Structure

```
cifar10-ml-final-project/
â”œâ”€â”€ run_all.py               # Main entry point
â”œâ”€â”€ requirements.txt         # Required Python packages
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ models/                  # Classical ML model training
â”‚   â””â”€â”€ train_classical_models.py
â”‚
â”œâ”€â”€ preprocess/              # Feature extraction modules
â”‚   â””â”€â”€ extract_features.py  # HOG + PCA
â”‚
â”œâ”€â”€ utils/                   # Evaluation and helper scripts
â”‚   â””â”€â”€ evaluation.py
â”‚
â”œâ”€â”€ report/                  # Results, plots, tuning logs (to be generated)
â””â”€â”€ data/                    # (Optional) Place to store dataset manually
```

---

## ğŸ“¦ Requirements

You can install all dependencies via:

```bash
pip install -r requirements.txt
```

### Main libraries used:
- `numpy`, `scikit-learn`, `matplotlib`, `pandas`
- `opencv-python` for HOG
- `torch`, `torchvision` for the CNN

---

## ğŸš€ How to Run the Project

After installing requirements:

```bash
python run_all.py
```

This will:
1. Run **HOG + PCA** on CIFAR-10 images
2. Train and evaluate:
   - Logistic Regression
   - K-Nearest Neighbors
   - Random Forest
   - Support Vector Machine
3. Train a **simple CNN baseline**
4. Output performance results and summary metrics

All results will be logged in the `report/` directory.

---

## ğŸ“‰ Dimensionality Reduction

- **PCA** is used after HOG to reduce dimensionality.
- `n_components=100` was chosen (explained in the report).
- This helps models train faster and generalize better.

---

## ğŸ§ª Models & Evaluation

Each classical model was evaluated using:
- **Accuracy**, **Precision**, **Recall**, **F1 Score**
- **Confusion Matrix** per class

Additionally:
- Each model was **fine-tuned** on its 2 most important hyperparameters.
- CNN was used **only as a baseline**, not as the projectâ€™s focus.

---

## ğŸ“Š Project Outputs

- Evaluation tables for all models (before/after PCA-HOG)
- Comparison to CNN
- Hyperparameter tuning logs
- Simulated full-dataset results (used due to resource limits)

---

## ğŸ’¡ Notes

- This project simulates full CIFAR-10 results due to memory constraints.
- All optimizations (PCA, HOG) are applied together.
- CNN is not optimized â€” it serves as a lower-bound reference.

---

## âœï¸ Report

The final report will include:
- Explanation of decisions, challenges, and optimizations
- Visualization of hyperparameter tuning
- Model performance summary table
- Discussion of results and insights per class

---

## ğŸ“š Credits

- CIFAR-10 dataset: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
- Instructor: *Professor Lee-Ad Gottlieb / School of Computer Science*
- Student: **Nevo Biton**

---

## ğŸ“¬ Questions?

Feel free to open an issue or contact [NevoBiton20](https://github.com/NevoBiton20).
